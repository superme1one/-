
回顾神经网络
![](asserts/Pasted%20image%2020250813230007.png)
### 普通神经网络的局限
![](asserts/Pasted%20image%2020250813230146.png)

### 自然语言类处理任务
大部分NLP任务都是“seq2seq”任务
- 对话、文本生成、翻译、语音识别、词性标注

特点
- 输入是任意个数的向量，输出是任意（可与输入不同）个数的向量
- 输入向量之间互相影响，向量的绝对、相对位置不同，输出也不同

### 普通神经网络为什么无法处理NLP任务
词性标注任务：输入和输出相同
![](asserts/Pasted%20image%2020250813230719.png)
![](asserts/Pasted%20image%2020250813230802.png)
- 向量之间没有任何影响

### 如何为神经网络引入上下文信息
滑动窗口
![](asserts/Pasted%20image%2020250813230913.png)
缺点：只考虑固定长度的上下文

循环神经网络
![](asserts/Pasted%20image%2020250813230942.png)
缺点：长期记忆容易缺失（很容易被后面的输入覆盖）、无法批量计算（一个一个输入进来，算力的利用率不是很高）

### 自注意力

Attention is all you need

上下文就是输入和其他输入的相关程度的分布
![](asserts/Pasted%20image%2020250813231435.png)


#### 自注意力实现
![](asserts/Pasted%20image%2020250813231551.png)

表为对于q 第一列来说，每个输入k与它的相关占比
对于文本生成一类的任务，输入只能向前考虑上下文，此时需要对注意力进行mask操作

![](asserts/Pasted%20image%2020250813234012.png)

![](asserts/Pasted%20image%2020250813234101.png)
X是整个输入

![](asserts/Pasted%20image%2020250813234620.png)


![](asserts/Pasted%20image%2020250813234757.png)

### 旋转位置编码
![](asserts/Pasted%20image%2020250813234930.png)

https://zhuanlan.zhihu.com/p/631363482

- 给输入直接加位置的数值
	- 上下文非常长，这个数值越来越大，f16数值不够用

- 位置编码应运而生
	- 运用了角度的概念
优点：
- 相对的位置
- 无限的位置信息

公式
- n是当前的位置
- d为k或q的向量长度
- rope_theta 10000 常量

逐个元素的计算

![](asserts/Pasted%20image%2020250814090745.png)

![](asserts/Pasted%20image%2020250814090810.png)
- 输入数量
### 多注意力
![](asserts/Pasted%20image%2020250814091159.png)

### 多层感知器（MLP）层
![](asserts/Pasted%20image%2020250814091428.png)
- 知识存在这层神经网络层

- 参数
	- 输入输出
	- 隐藏层数量
	- 


### 解码器（Decoder）模块
![](asserts/Pasted%20image%2020250814091848.png)
### 如何构建词表
![](asserts/Pasted%20image%2020250814100517.png)
### Tokenizer
标记器
- Encode：根据词表，将文本转换为token-id序列
- Decode：将token-id还原成文本
![](asserts/Pasted%20image%2020250814101009.png)
这个过程不是互逆的

### 文本嵌入层
![](asserts/Pasted%20image%2020250814101106.png)

- 序列转换为向量
- One-hot
	- 最简单的用种类去代表，向量长度和词表一样长，词表会非常巨大，向量非常长
	- 不带任何语义
### 大模型结构
![](asserts/Pasted%20image%2020250814101925.png)
